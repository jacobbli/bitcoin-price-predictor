{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('test_env': venv)"
  },
  "interpreter": {
   "hash": "207a1d4264d7b30b581ef8e9e1fab3771fe2b2daf687a49f3b6a80360b97736a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1.0 Compiled Workflow\r\n",
    "\r\n",
    "This Jupyter notebook contains the complete Python workflow. We have also prepared 5 other Jupyter notebooks that break the workflow in this notebook into distinct parts. The code in this notebook and the broken up notebooks is identical except for code that imports/exports .csv files.\r\n",
    "\r\n",
    "We felt that, although this notebook is useful for generating all the datasets needed for our analysis without having to switch between notebooks, it might be difficult to mark due to its length. Showing our workflow in separate, shorter, notebooks might make it easier to mark for the teaching team."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# !pip install pandas"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Declare functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def is_profitable(row, threshold):\r\n",
    "    if row['next_close'] >= (row['Close']*(threshold)):\r\n",
    "        row['is_profitable'] = 'PROFITABLE'\r\n",
    "    else:\r\n",
    "        row['is_profitable'] = \"UNPROFITABLE\"\r\n",
    "    return row"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# positive = 1\r\n",
    "# negative = -1\r\n",
    "# neutral = 0\r\n",
    "\r\n",
    "def determine_emotion(row):\r\n",
    "    if row['pos_emo_y'] > row['neg_emo_y']:\r\n",
    "        row['top_influencer_emotion'] = 1\r\n",
    "    elif row['pos_emo_y'] < row['neg_emo_y']:\r\n",
    "        row['top_influencer_emotion'] = -1\r\n",
    "    else:\r\n",
    "        row['top_influencer_emotion'] = 0\r\n",
    "    return row"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set parameters\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "sma_range = 10\r\n",
    "ema_range = 10\r\n",
    "ema_smoothing = 2\r\n",
    "\r\n",
    "profitable_threshold = 1.02"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import datasets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "prices_df = pd.read_csv('../original-datasets/bitcoin.csv')\r\n",
    "reddit_df = pd.read_csv('../original-datasets/RedditCrypto-2017.csv', header=0, names=['parent_thread_id', 'post_id', 'user_id', 'timestamp', 'pos_emo', 'neg_emo'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "prices_df['Date'] = pd.to_datetime(prices_df['Date'])\r\n",
    "prices_df.sort_values(by='Date', inplace=True)\r\n",
    "prices_df = prices_df.reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 Generate labels Bitcoin prices dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "prices_df['next_close'] = prices_df['Close'].shift(-1)\r\n",
    "prices_df = prices_df.apply(is_profitable, axis=1, threshold=profitable_threshold)\r\n",
    "prices_df.drop('next_close', axis=1, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Derive attributes for Bitcoin prices dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculate simple moving average"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "prices_df['SMA'] = prices_df['Close'].rolling(sma_range).mean()"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculate exponential moving average"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "prices_df['EMA'] = prices_df['Close']\r\n",
    "prices_df.loc[ema_range-1, 'EMA'] = (prices_df.iloc[ema_range-1])['SMA']\r\n",
    "prices_df['EMA'] = prices_df.iloc[9:,9].ewm(span=ema_range,adjust=False).mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calulate 2-day price differences"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "prices_df['close-low'] = prices_df.apply(lambda x: x['Close']-x['Low'], axis=1)\r\n",
    "prices_df['close-high'] = prices_df.apply(lambda x: x['Close']-x['High'], axis=1)\r\n",
    "prices_df['SMA-close'] = prices_df.apply(lambda x: x['SMA']-x['Close'], axis=1)\r\n",
    "prices_df['EMA-close'] = prices_df.apply(lambda x: x['EMA']-x['Close'], axis=1)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "prices_df['prev_EMA'] = prices_df['EMA'].shift(1)\r\n",
    "prices_df['EMA_diff'] = prices_df['EMA'] - prices_df['prev_EMA']\r\n",
    "prices_df.drop('prev_EMA', axis=1, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "prices_df['prev_SMA'] = prices_df['SMA'].shift(1)\r\n",
    "prices_df['SMA_diff'] = prices_df['SMA'] - prices_df['prev_SMA']\r\n",
    "prices_df.drop('prev_SMA', axis=1, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Remove tuples with NA values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "prices_df.dropna(inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Drop unused columns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "prices_df.drop(['Open','High','Low','Close','Volume','Market Cap', 'SMA', 'EMA'], axis=1, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 Process Reddit dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "reddit_classification_df = reddit_df[['timestamp', 'pos_emo', 'neg_emo']].copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convert date column"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "reddit_classification_df['timestamp'] = pd.to_datetime(reddit_classification_df['timestamp'])\r\n",
    "reddit_classification_df['date'] = reddit_classification_df['timestamp'].dt.date"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Aggregate sum on date column"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "reddit_classification_df = reddit_classification_df.groupby(['date'], as_index=False).sum()\r\n",
    "reddit_classification_df['date'] = pd.to_datetime(reddit_classification_df['date'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Combine reddit dataframe and historical prices dataframe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "classification_2017 = prices_df.merge(reddit_classification_df, left_on='Date', right_on='date')\r\n",
    "classification_2017.drop('date', axis=1, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.4 Prepare network analysis dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "reddit_sna_df = reddit_df.copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Remove leading and trailing white space"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "reddit_sna_df['parent_thread_id'] = reddit_sna_df['parent_thread_id'].str.strip()\r\n",
    "reddit_sna_df['post_id'] = reddit_sna_df['post_id'].str.strip()\r\n",
    "reddit_sna_df['user_id'] = reddit_sna_df['user_id'].str.strip()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract thread id from file name"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "reddit_sna_df['parent_thread_id'] = reddit_sna_df['parent_thread_id'].str.slice(start=15, stop=21)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create dataframe with only \"parent\" posts"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "creator_posts_df = reddit_sna_df[reddit_df['post_id'] == reddit_sna_df['parent_thread_id']]\r\n",
    "creator_posts_df = creator_posts_df.rename(columns={'user_id': 'creator_id', 'timestamp': 'thread_creation_date'})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Join the \"parent posts\" dataframe with the original dataset\n",
    "\n",
    "Match each post in the complete dataset with its corresponding \"parent post\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "joined_posts_df = creator_posts_df.merge(reddit_sna_df, left_on='post_id', right_on='parent_thread_id')\r\n",
    "joined_posts_df.drop(['post_id_x', 'pos_emo_x', 'neg_emo_x', 'parent_thread_id_y'], axis=1, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Remove tuples with the same creator_id and user_id"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "joined_posts_df = joined_posts_df[joined_posts_df['creator_id'] != joined_posts_df['user_id']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Remove tuples where at least one of the users involved are bots"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "joined_posts_df = joined_posts_df[joined_posts_df['creator_id'] != 'AutoModerator']\r\n",
    "joined_posts_df = joined_posts_df[joined_posts_df['creator_id'] != 'None']\r\n",
    "joined_posts_df = joined_posts_df[joined_posts_df['user_id'] != 'None']\r\n",
    "joined_posts_df = joined_posts_df[joined_posts_df['user_id'] != 'AutoModerator']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Rename columns for clarity"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "joined_posts_df = joined_posts_df.rename(\r\n",
    "    columns={\r\n",
    "        'parent_thread_id_x': 'parent_thread_id', \r\n",
    "        'post_id_y': 'post_id', \r\n",
    "        'timestamp': 'post_creation_date', \r\n",
    "        'pos_emo_y': 'pos_emo', \r\n",
    "        'neg_emo_y': 'neg_emo',\r\n",
    "        'user_id': 'replier_id'\r\n",
    "    })"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reorder columns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "joined_posts_df = joined_posts_df[['parent_thread_id', 'thread_creation_date', 'post_id', 'post_creation_date', 'creator_id', 'replier_id', 'pos_emo', 'neg_emo']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "classification_2017.to_csv('../generated-datasets/classification-dataset-with-emotion-scores.csv')\r\n",
    "prices_df.to_csv('../generated-datasets/classification-dataset.csv')\r\n",
    "joined_posts_df.to_csv('../generated-datasets/network-analysis-dataset.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NOTE: Please execute the KNIME workflow in `2-social-network-analysis-workflow.knwf` before continuing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare top influencer classification dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import and preprocess datasets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "influencers_df = pd.read_csv('../generated-datasets/top-influencers.csv', usecols=['Object id']).head(3)\r\n",
    "\r\n",
    "reddit_df = pd.read_csv('../original-datasets/RedditCrypto-2017.csv', header=0, usecols=['Source (C)', 'Source (E)', 'posemo', 'negemo'])\r\n",
    "reddit_df = reddit_df.rename(columns={'Source (C)': 'poster', 'Source (E)':'timestamp', 'posemo': 'pos_emo', 'negemo': 'neg_emo'})\r\n",
    "reddit_df['poster'] = reddit_df['poster'].str.strip()\r\n",
    "\r\n",
    "prices_df = pd.read_csv('../generated-datasets/classification-dataset-with-emotion-scores.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Keep only posts by top influencers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "reddit_df = reddit_df.merge(right=influencers_df, left_on='poster', right_on='Object id')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Take aggreagate sum of each day"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "reddit_df['timestamp'] = pd.to_datetime(reddit_df['timestamp'])\r\n",
    "reddit_df['timestamp'] = reddit_df['timestamp'].dt.date\r\n",
    "\r\n",
    "reddit_df = reddit_df.groupby(['timestamp'], as_index=False).sum()\r\n",
    "reddit_df['timestamp'] = pd.to_datetime(reddit_df['timestamp'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Join top influencer posts with other daily statistics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "prices_df['Date'] = pd.to_datetime(prices_df['Date'])\r\n",
    "\r\n",
    "reddit_df = pd.merge(prices_df, reddit_df, how='left', left_on=[\"Date\"], right_on=[\"timestamp\"], sort=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create column indicating the dominating emotion of the posts from the top influencers for each day\r\n",
    "negative = -1\r\n",
    "\r\n",
    "neutral = 0\r\n",
    "\r\n",
    "positive = 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "reddit_df = reddit_df.apply(determine_emotion, axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Drop unused columns and rename column headers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "reddit_df = reddit_df.drop(columns=['Unnamed: 0', 'timestamp', 'pos_emo_y', 'neg_emo_y'])\r\n",
    "reddit_df = reddit_df.rename(columns={'pos_emo_x': 'pos_emo', 'neg_emo_x': 'neg_emo'})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Export datasets to csv"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "reddit_df.to_csv('../generated-datasets/classification-dataset-with-emotion-scores.csv')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}